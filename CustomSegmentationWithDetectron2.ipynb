{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["##Ä°nstall Detectron2"],"metadata":{"id":"kwH1_zTzjuVC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgJvQ4g2bh8I"},"outputs":[],"source":["!python -m pip install pyyaml==5.1\n","import sys, os, distutils.core\n","# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities.\n","# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n","!git clone 'https://github.com/facebookresearch/detectron2'\n","dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n","!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n","sys.path.insert(0, os.path.abspath('./detectron2'))\n","\n","# Properly install detectron2. (Please do not install twice in both ways)\n","# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"]},{"cell_type":"code","source":["import torch, detectron2\n","!nvcc --version\n","TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n","CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n","print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n","print(\"detectron2:\", detectron2.__version__)"],"metadata":{"id":"op2oYQQybyu0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Some basic setup:\n","# Setup detectron2 logger\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","# import some common libraries\n","import numpy as np\n","import os, json, cv2, random\n","from google.colab.patches import cv2_imshow\n","\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","import pickle\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","from detectron2.structures import BoxMode\n","from detectron2.utils.visualizer import ColorMode\n","\n","\n","# import some common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog"],"metadata":{"id":"bHRLkAkhby1-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Dataset Preparing"],"metadata":{"id":"CGPO2SUCj6FE"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"98KZ7Wqpby8A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/fruit_segmentation/fruit_dataset.zip > /dev/null"],"metadata":{"id":"d6dS1iZ9cAJv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_dicts(img_dir,classes):\n","    \"\"\"This function loads the JSON file created with the annotator and converts it to\n","    the detectron2 metadata specifications.\n","    \"\"\"\n","    # load the JSON file\n","    json_file = os.path.join(img_dir, \"via_project_data.json\")\n","    with open(json_file) as f:\n","        imgs_anns = json.load(f)\n","\n","    dataset_dicts = []\n","    # loop through the entries in the JSON file\n","    for idx, v in enumerate(imgs_anns.values()):\n","        record = {}\n","        # add file_name, image_id, height and width information to the records\n","        filename = os.path.join(img_dir, v[\"filename\"])\n","        height, width = cv2.imread(filename).shape[:2]\n","\n","        record[\"file_name\"] = filename\n","        record[\"image_id\"] = idx\n","        record[\"height\"] = height\n","        record[\"width\"] = width\n","\n","        annos = v[\"regions\"]\n","        print(annos)\n","\n","        objs = []\n","        # one image can have multiple annotations, therefore this loop is needed\n","        for annotation in annos:\n","            # reformat the polygon information to fit the specifications\n","            anno = annotation[\"shape_attributes\"]\n","            px = anno[\"all_points_x\"]\n","            py = anno[\"all_points_y\"]\n","            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n","            poly = [p for x in poly for p in x]\n","\n","            region_attributes = annotation[\"region_attributes\"][\"type\"]\n","\n","            # specify the category_id to match with the class.\n","           \n","            obj = {\n","                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n","                \"bbox_mode\": BoxMode.XYXY_ABS,\n","                \"segmentation\": [poly],\n","                \"category_id\": classes.index(region_attributes),\n","                \"iscrowd\": 0,\n","            }\n","            objs.append(obj)\n","        record[\"annotations\"] = objs\n","        dataset_dicts.append(record)\n","\n","    return dataset_dicts"],"metadata":{"id":"-kD3xtptcAMY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classes = ['apple', 'grape','lemon','orange','strawberry']\n","\n","for d in [\"train\", \"val\"]:\n","    DatasetCatalog.register(\n","        \"fruit_\" + d,\n","        lambda d=d: get_dicts(\"/content/fruit_dataset/\" + d,classes),\n","    )\n","\n","metadata_train = MetadataCatalog.get(\"fruit_train\").set(thing_classes=classes)\n","metadata_val = MetadataCatalog.get(\"fruit_val\").set(thing_classes=classes)\n","\n","dataset_dicts = get_dicts(\"/content/fruit_dataset/train\",classes)"],"metadata":{"id":"Aye_yxsWcAP6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Training"],"metadata":{"id":"Bve2QgjQkiSM"}},{"cell_type":"code","source":["cfg = get_cfg()\n","# you can choose alternative models as backbone here\n","cfg.merge_from_file(model_zoo.get_config_file(\n","    \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"\n","))\n","\n","cfg.DATASETS.TRAIN = (\"fruit_train\",)\n","cfg.DATASETS.TEST = ()\n","cfg.DATALOADER.NUM_WORKERS = 0\n","# if you changed the model above, you need to adapt the following line as well\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n","    \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"\n",")  # Let training initialize from model zoo\n","cfg.SOLVER.IMS_PER_BATCH = 4\n","cfg.SOLVER.BASE_LR = 0.00025\n","cfg.SOLVER.CHECKPOINT_PERIOD = 100\n","\n","cfg.TEST.EVAL_PERIOD = 100\n","cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = (\n","    512  # (default: 512), select smaller if faster training is needed\n",")\n","cfg.SOLVER.MAX_ITER = 1000 \n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5  "],"metadata":{"id":"8gnUvjUAgsPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from detectron2.engine import DefaultTrainer\n","resume_dir = os.getcwd()+'/output/'\n","os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n","trainer = DefaultTrainer(cfg)\n","trainer.resume_or_load(resume=False)"],"metadata":{"id":"__J2LLkwgsTE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"v3Ag3r4CoVIR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Load Results"],"metadata":{"id":"JTdVYGd3kx2S"}},{"cell_type":"code","source":["%cp /content/output/last_checkpoint /content/drive/MyDrive/fruit_segmentation/output\n","%cp /content/output/metrics.json /content/drive/MyDrive/fruit_segmentation/output\n","%cp /content/output/model_0000999.pth /content/drive/MyDrive/fruit_segmentation/output"],"metadata":{"id":"9gpwm10woVLs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Look at training curves in tensorboard:\n","%load_ext tensorboard\n","%tensorboard --logdir output"],"metadata":{"id":"gVZ7c2W9ow8d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir predictions\n","!mkdir output_images"],"metadata":{"id":"hmtX6URIow_W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Prediction"],"metadata":{"id":"Wfo4WY0clgTe"}},{"cell_type":"code","source":["#cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n","cfg.MODEL.WEIGHTS = \"/content/drive/MyDrive/fruit_segmentation/output/model_0000999.pth\"\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = (\n","    0.75  # set the testing threshold for this model\n",")\n","\n","# load the validation data\n","cfg.DATASETS.TEST = (\"fruit_train\",)\n","# create a predictor\n","predictor = DefaultPredictor(cfg)\n","\n","start = datetime.now()\n","\n","validation_folder = Path(\"/content/fruit_dataset/val\")\n","#validation_folder = Path(\"/content/drive/MyDrive/detectron/test\")\n","for i, file in enumerate(validation_folder.glob(\"*.jpg\")):\n","    # this loop opens the .jpg files from the val-folder, creates a dict with the file\n","    # information, plots visualizations and saves the result as .pkl files.\n","    file = str(file)\n","    file_name = file.split(\"/\")[-1]\n","    im = cv2.imread(file)\n","\n","    outputs = predictor(im)\n","    output_with_filename = {}\n","    output_with_filename[\"file_name\"] = file_name\n","    output_with_filename[\"file_location\"] = file\n","    output_with_filename[\"prediction\"] = outputs\n","    # the following two lines save the results as pickle objects, you could also\n","    # name them according to the file_name if you want to keep better track of your data\n","    with open(f\"/content/predictions/predictions_{i}.pkl\", \"wb\") as f:\n","        pickle.dump(output_with_filename, f)\n","    v = Visualizer(\n","        im[:, :, ::-1],\n","        metadata=metadata_train,\n","        scale=0.5,\n","        #instance_mode=ColorMode.IMAGE_BW,  # remove the colors of unsegmented pixels\n","    )\n","\n","    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","    plt.imshow(v.get_image()[:, :, ::-1])\n","    plt.savefig(f\"/content/output_images/{file_name}\")\n","print(\"Time needed for inferencing:\", datetime.now() - start)"],"metadata":{"id":"wepu7Qm3oxCx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prediction for video\n"],"metadata":{"id":"-nBNgfj7lpKU"}},{"cell_type":"code","source":["import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","# import some common libraries\n","import numpy as np\n","import tqdm\n","import cv2\n","# import some common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.video_visualizer import VideoVisualizer\n","from detectron2.utils.visualizer import ColorMode, Visualizer\n","from detectron2.data import MetadataCatalog\n","import time\n","\n","# Extract video properties\n","video = cv2.VideoCapture('/video.mp4')##video filepath\n","width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n","height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","frames_per_second = video.get(cv2.CAP_PROP_FPS)\n","num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","# Initialize video writer\n","video_writer = cv2.VideoWriter('out.mp4', fourcc=cv2.VideoWriter_fourcc(*\"mp4v\"), fps=float(frames_per_second), frameSize=(width, height), isColor=True)\n","\n","# Initialize predictor\n","\n","cfg.MODEL.WEIGHTS = \"/content/drive/MyDrive/detectron2/model_final.pth\"\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = (\n","    0.70  # set the testing threshold for this model\n",")\n","\n","# load the validation data\n","cfg.DATASETS.TEST = (\"fruit_val\",)\n","# create a predictor\n","predictor = DefaultPredictor(cfg)\n","\n","# Initialize visualizer\n","v = VideoVisualizer(MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), ColorMode.IMAGE)\n","\n","def runOnVideo(video, maxFrames):\n","    \"\"\" Runs the predictor on every frame in the video (unless maxFrames is given),\n","    and returns the frame with the predictions drawn.\n","    \"\"\"\n","\n","    readFrames = 0\n","    while True:\n","        hasFrame, frame = video.read()\n","        if not hasFrame:\n","            break\n","\n","        # Get prediction results for this frame\n","        outputs = predictor(frame)\n","        # Make sure the frame is colored\n","        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n","\n","        # Draw a visualization of the predictions using the video visualizer\n","        visualization = v.draw_instance_predictions_seg(frame, outputs[\"instances\"].to(\"cpu\"))\n","\n","        # Convert Matplotlib RGB format to OpenCV BGR format\n","        visualization = cv2.cvtColor(visualization.get_image(), cv2.COLOR_RGB2BGR)\n","\n","        yield visualization\n","\n","        readFrames += 1\n","        if readFrames > maxFrames:\n","            break\n","\n","# Create a cut-off for debugging\n","num_frames = 1070\n","\n","# Enumerate the frames of the video\n","for visualization in tqdm.tqdm(runOnVideo(video, num_frames), total=num_frames):\n","\n","    # Write test image\n","    cv2.imwrite('POSE detectron2.png', visualization)\n","\n","    # Write to video file\n","    video_writer.write(visualization)\n","\n","# Release resources\n","video.release()\n","video_writer.release()\n","cv2.destroyAllWindows()"],"metadata":{"id":"Y04JqfHjo7G-"},"execution_count":null,"outputs":[]}]}